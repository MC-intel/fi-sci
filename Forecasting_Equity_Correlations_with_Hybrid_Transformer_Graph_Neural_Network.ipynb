{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": "# Forecasting Equity Correlations with Hybrid Transformer Graph Neural Network (THGNN)\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/MC-intel/fi-sci/blob/main/Forecasting_Equity_Correlations_with_Hybrid_Transformer_Graph_Neural_Network.ipynb)\n\n## Paper Implementation Demo\n**Reference:** arXiv:2601.04602v1 - Fanshawe, Masih, Cameron (University of Queensland)\n\n### Key Concepts from the Paper:\n1. **Problem:** Traditional StatArb uses backward-looking rolling correlations that lag during regime shifts\n2. **Solution:** THGNN combines Transformer (temporal) + GAT (relational) to predict future correlations\n3. **Innovation:** Predictions in Fisher-z space with residual learning from rolling baseline\n\n### Architecture Overview:\n```\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚  Per-Stock Features (37 features Ã— 30 days)                     â”‚\nâ”‚                          â†“                                       â”‚\nâ”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚\nâ”‚  â”‚  TEMPORAL ENCODER (Transformer)                          â”‚    â”‚\nâ”‚  â”‚  - 4 layers, 8 heads, d_model=128                        â”‚    â”‚\nâ”‚  â”‚  - Captures time-series patterns per stock               â”‚    â”‚\nâ”‚  â”‚  - Output: 512-dim node embedding                        â”‚    â”‚\nâ”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚\nâ”‚                          â†“                                       â”‚\nâ”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚\nâ”‚  â”‚  RELATIONAL ENCODER (Graph Attention Network)            â”‚    â”‚\nâ”‚  â”‚  - 3 layers, 4 heads per layer                           â”‚    â”‚\nâ”‚  â”‚  - Stocks exchange info with correlated neighbors        â”‚    â”‚\nâ”‚  â”‚  - Edge-aware attention (correlation strength/sign)      â”‚    â”‚\nâ”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚\nâ”‚                          â†“                                       â”‚\nâ”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚\nâ”‚  â”‚  EXPERT HEADS (Relation-Routed)                          â”‚    â”‚\nâ”‚  â”‚  - 3 MLPs: neg/mid/pos correlation regimes               â”‚    â”‚\nâ”‚  â”‚  - Output: Î”z (Fisher-z residual)                        â”‚    â”‚\nâ”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚\nâ”‚                          â†“                                       â”‚\nâ”‚  z_pred = z_baseline + Î”z  â†’  Ï_pred = tanh(z_pred)             â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n```\n\n---"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 989
    },
    "id": "K6tSykPn65NI",
    "outputId": "5fc25f7a-adc5-4136-c01d-145c9d30d644"
   },
   "outputs": [],
   "source": "# =============================================================================\n# CELL 1: ENVIRONMENT SETUP & CORE CONCEPTS\n# =============================================================================\n# This cell installs dependencies and demonstrates the fundamental concept:\n# Treating the stock market as a GRAPH where stocks are nodes and correlations\n# are edges - enabling Graph Neural Networks to learn market structure.\n#\n# PAPER REFERENCE (Section 3.3):\n# \"Graph Neural Networks provide a natural fit for financial markets because\n# they model the market-wide network structure rather than isolated pairs.\"\n# =============================================================================\n\n# --- Install PyTorch Geometric (GNN library) ---\n# PyG is the standard library for Graph Neural Networks in PyTorch\n!pip install -q yfinance torch_geometric\n\n# --- Core Imports ---\nimport yfinance as yf          # Fetch live stock data\nimport torch                   # Deep learning framework\nimport torch.nn as nn          # Neural network modules\nimport torch.nn.functional as F # Activation functions, losses\nfrom torch_geometric.nn import GATConv  # Graph Attention layer\nimport networkx as nx          # Graph visualization\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\n\n# =============================================================================\n# DEVICE CONFIGURATION\n# =============================================================================\n# The paper trained on GPU; this code auto-detects available hardware\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"ğŸ–¥ï¸  Running on: {device}\")\nprint(\"=\" * 60)\n\n# =============================================================================\n# SECTION 1: THE MARKET AS A GRAPH (Multi-Agent Environment)\n# =============================================================================\n# PAPER INSIGHT (Section 3.3):\n# \"Stocks are nodes and edges represent current/lagged information, such as\n# past or predicted correlations, common factor exposures, sector membership,\n# or other meaningful economic connections.\"\n#\n# WHY GRAPHS?\n# - Traditional methods treat stock pairs independently\n# - Graph-based methods capture NETWORK STRUCTURE\n# - Two stocks may have weak direct correlation but strong indirect links\n# - This is crucial for basket trading and clustering\n# =============================================================================\n\n# --- Define our \"mini S&P 500\" universe ---\n# Paper uses full S&P 500; we use 8 representative stocks for demonstration\ntickers = ['NVDA', 'AMD', 'INTC',      # Semiconductor sector\n           'MSFT', 'GOOGL',             # Cloud/Software sector\n           'AAPL', 'TSLA', 'AMZN']      # Consumer Tech/Retail\n\nprint(f\"ğŸ“Š Fetching live data for {len(tickers)} stocks...\")\nprint(f\"   Tickers: {tickers}\")\ndata = yf.download(tickers, period=\"6mo\", interval=\"1d\", progress=False)['Close']\nreturns = data.pct_change().dropna()\nprint(f\"   Data shape: {returns.shape} (days Ã— stocks)\")\n\n# =============================================================================\n# GRAPH CONSTRUCTION\n# =============================================================================\n# PAPER REFERENCE (Section 4.2.3 - Graph Building):\n# \"For every stock the top 50, bottom 50 correlations, and 75 randomly sampled\n# mid-strength partners are included as weighted edges.\"\n#\n# For this demo, we manually define edges based on known relationships:\n# - Sector membership (chips cluster together)\n# - Supply chain links (AAPL buys from NVDA)\n# - Business partnerships (MSFT-NVDA AI collaboration)\n# - Competition (AMZN-MSFT cloud rivalry)\n#\n# In production, edges would be computed from correlation matrices.\n# =============================================================================\n\n# Edge definition: [source, target] pairs (undirected = bidirectional)\n# Index mapping: 0:NVDA, 1:AMD, 2:INTC, 3:MSFT, 4:GOOGL, 5:AAPL, 6:TSLA, 7:AMZN\nedge_index = torch.tensor([\n    # --- SEMICONDUCTOR CLUSTER (Strong positive correlation expected) ---\n    [0, 1], [1, 0],  # NVDA <-> AMD (direct competitors)\n    [1, 2], [2, 1],  # AMD <-> INTC (x86 competitors)\n    [0, 2], [2, 0],  # NVDA <-> INTC (GPU vs CPU)\n    \n    # --- CLOUD/SOFTWARE CLUSTER ---\n    [3, 4], [4, 3],  # MSFT <-> GOOGL (cloud competitors)\n    \n    # --- CROSS-SECTOR SUPPLY CHAIN LINKS ---\n    [5, 0], [0, 5],  # AAPL <-> NVDA (Apple uses NVIDIA GPUs)\n    [3, 0], [0, 3],  # MSFT <-> NVDA (AI partnership: Azure + NVIDIA)\n    \n    # --- BUSINESS RIVALRY LINKS ---\n    [7, 3], [3, 7],  # AMZN <-> MSFT (AWS vs Azure)\n    [6, 0], [0, 6],  # TSLA <-> NVDA (autonomous driving chips)\n], dtype=torch.long).t().contiguous().to(device)\n# Note: .t() transposes to PyG format: [2, num_edges]\n\nprint(f\"\\nğŸ”— Graph constructed:\")\nprint(f\"   Nodes: {len(tickers)} stocks\")\nprint(f\"   Edges: {edge_index.shape[1]} connections\")\n\n# =============================================================================\n# GRAPH VISUALIZATION\n# =============================================================================\ndef visualize_market_graph():\n    \"\"\"\n    Visualize the stock market as a network graph.\n    \n    This demonstrates the paper's key insight: stocks are not independent\n    but form an interconnected network where information propagates.\n    \"\"\"\n    plt.figure(figsize=(10, 8))\n    G = nx.Graph()\n    \n    # Add nodes with sector colors\n    sector_colors = {\n        'NVDA': '#76B900', 'AMD': '#ED1C24', 'INTC': '#0071C5',  # Chips\n        'MSFT': '#00A4EF', 'GOOGL': '#4285F4',                    # Cloud\n        'AAPL': '#A2AAAD', 'TSLA': '#CC0000', 'AMZN': '#FF9900'  # Consumer\n    }\n    \n    for ticker in tickers:\n        G.add_node(ticker)\n    \n    # Add edges\n    edges_list = edge_index.cpu().t().numpy()\n    for u, v in edges_list:\n        if u < v:  # Avoid duplicate edges\n            G.add_edge(tickers[u], tickers[v])\n    \n    pos = nx.spring_layout(G, seed=42, k=2)\n    colors = [sector_colors[t] for t in tickers]\n    \n    nx.draw(G, pos, with_labels=True, node_color=colors,\n            node_size=3000, font_size=12, font_weight='bold',\n            edge_color='gray', width=2, alpha=0.9)\n    \n    plt.title(\"Market Structure as a Graph\\n(Nodes=Stocks, Edges=Correlations/Relationships)\",\n              fontsize=14, fontweight='bold')\n    plt.figtext(0.5, 0.02, \n                \"Paper Insight: GNNs capture network effects that pairwise correlations miss\",\n                ha='center', fontsize=10, style='italic')\n    plt.tight_layout()\n    plt.show()\n\nvisualize_market_graph()\n\n# =============================================================================\n# SIMPLIFIED THGNN MODEL (Concept Demonstration)\n# =============================================================================\n# This is a simplified version to demonstrate the TWO-STAGE architecture:\n#\n# STAGE 1 - TEMPORAL ENCODER (Transformer):\n#   \"The Transformer processes each stock's historical feature sequence to\n#    produce latent node embeddings\" (Paper Section 4.2.2)\n#\n# STAGE 2 - RELATIONAL ENCODER (GAT):\n#   \"The graph module models interactions through edge attributes such as\n#    pairwise correlation\" (Paper Section 4.2.4)\n#\n# Paper Architecture (Full):              Our Demo (Simplified):\n# - d_model = 128                         - d_model = 32\n# - 4 Transformer layers                  - 1 Transformer layer\n# - 8 attention heads                     - 4 attention heads\n# - 3 GAT layers, 4 heads each            - 1 GAT layer, 1 head\n# - 512-dim embeddings                    - 32-dim embeddings\n# =============================================================================\n\nclass SimplifiedTHGNN(nn.Module):\n    \"\"\"\n    Simplified Temporal-Heterogeneous Graph Neural Network.\n    \n    This demonstrates the paper's core insight: combining temporal patterns\n    (Transformer) with relational structure (GAT) for correlation prediction.\n    \n    PAPER QUOTE (Section 3.5):\n    \"GNNs specialize in learning the spatial structure of market relationships,\n    while Transformers specialize in modeling the temporal evolution of those\n    relationships.\"\n    \"\"\"\n    def __init__(self, window_size, d_model=32):\n        super().__init__()\n        \n        # =====================================================================\n        # STAGE 1: TEMPORAL ENCODER (Transformer)\n        # =====================================================================\n        # PURPOSE: Encode each stock's price history into a rich embedding\n        # \n        # Paper (Section 4.2.2): \"For every stock i at date t, we form a\n        # sequence X(i,t) from the previous L=30 trading days of F=37 features\"\n        # =====================================================================\n        self.input_proj = nn.Linear(window_size, d_model)\n        self.transformer = nn.TransformerEncoderLayer(\n            d_model=d_model,    # Paper uses 128\n            nhead=4,            # Paper uses 8\n            batch_first=True,\n            dropout=0.2         # Paper uses 0.2\n        )\n        \n        # =====================================================================\n        # STAGE 2: RELATIONAL ENCODER (Graph Attention Network)\n        # =====================================================================\n        # PURPOSE: Let stocks \"talk\" to their neighbors to update embeddings\n        #\n        # Paper (Section 4.2.4): \"A graph attention network models cross-asset\n        # interactions... allowing each stock to selectively aggregate\n        # information from economically relevant neighbors.\"\n        #\n        # Key Innovation: GAT learns WHICH neighbors matter most via attention\n        # =====================================================================\n        self.gat = GATConv(d_model, d_model, heads=1)\n        \n        # =====================================================================\n        # PREDICTION HEAD\n        # =====================================================================\n        # In the paper, this predicts Î”z (Fisher-z residual)\n        # Here simplified to predict next return direction\n        # =====================================================================\n        self.head = nn.Linear(d_model, 1)\n    \n    def forward(self, x, edge_index):\n        \"\"\"\n        Forward pass through the THGNN.\n        \n        Args:\n            x: Stock features [num_stocks, window_size]\n            edge_index: Graph edges [2, num_edges]\n        \n        Returns:\n            predictions: [num_stocks, 1]\n        \"\"\"\n        # --- TEMPORAL PASS: Each stock processes its own history ---\n        # \"The Transformer produces context-aware, asset-specific embeddings\n        # from the full T-day input window\" (Paper Section 3.5)\n        x = self.input_proj(x)          # [num_stocks, d_model]\n        x = x.unsqueeze(1)              # [num_stocks, 1, d_model]\n        x = self.transformer(x)          # [num_stocks, 1, d_model]\n        x = x.squeeze(1)                # [num_stocks, d_model]\n        \n        # --- RELATIONAL PASS: Stocks exchange information ---\n        # \"The GAT then focuses on the relational structure between assets,\n        # weighting neighbors via learned attention\" (Paper Section 3.5)\n        x = F.relu(self.gat(x, edge_index))\n        \n        # --- PREDICTION ---\n        return self.head(x)\n\n# =============================================================================\n# MODEL DEMONSTRATION\n# =============================================================================\nprint(\"\\n\" + \"=\" * 60)\nprint(\"SIMPLIFIED THGNN MODEL DEMONSTRATION\")\nprint(\"=\" * 60)\n\n# Prepare data: last 20 days of returns for all stocks\nWINDOW = 20\ntensor_data = torch.tensor(returns.values, dtype=torch.float32).to(device)\ncurrent_state = tensor_data[-WINDOW:].T  # [num_stocks, window_size]\n\n# Initialize and run model\nmodel = SimplifiedTHGNN(window_size=WINDOW).to(device)\n\nwith torch.no_grad():\n    predictions = model(current_state, edge_index)\n\n# Display results\nprint(f\"\\n{'STOCK':<10} | {'PREDICTED SIGNAL':<15}\")\nprint(\"-\" * 30)\nfor i, pred in enumerate(predictions):\n    signal = \"BULLISH\" if pred.item() > 0 else \"BEARISH\"\n    print(f\"{tickers[i]:<10} | {pred.item():>+.4f} ({signal})\")\n\nprint(\"\\nğŸ“ NOTE: These embeddings (before prediction head) would feed into\")\nprint(\"   SPONGEsym clustering for basket construction in the full paper.\")"
  },
  {
   "cell_type": "code",
   "source": "# =============================================================================\n# CELL 2: FULL THGNN IMPLEMENTATION WITH TRAINING\n# =============================================================================\n# This cell implements a more complete version of the paper's architecture\n# with proper feature engineering, Fisher-z transform, and training loop.\n#\n# KEY PAPER CONCEPTS IMPLEMENTED:\n# 1. Multi-feature input (Returns, Volatility, Momentum) - Paper uses 37 features\n# 2. Fisher-z space for correlation prediction\n# 3. Huber loss (robust to outliers)\n# 4. Proper train/test split for backtesting\n#\n# PAPER REFERENCE (Section 4.1):\n# \"We target 10-day-ahead correlations among S&P 500 constituents, aligned\n# with the 10-day rebalance period in the strategy.\"\n# =============================================================================\n\n# --- Re-import for standalone execution ---\n!pip install -q yfinance torch_geometric matplotlib pandas numpy seaborn\n\nimport yfinance as yf\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch_geometric.nn import GATConv\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# =============================================================================\n# CONFIGURATION (Paper-Aligned Parameters)\n# =============================================================================\n# Paper uses: 30-day window, 10-day prediction horizon, full S&P 500\n# Demo uses: Smaller scale for fast execution\n\nTICKERS = ['NVDA', 'AMD', 'INTC', 'MSFT', 'GOOGL', 'AAPL', 'TSLA', 'AMZN']\nBENCHMARK = 'QQQ'           # Benchmark for comparison\nLOOKBACK_WINDOW = 30        # Paper: \"L = 30 trading days\" (Section 4.2.2)\nPREDICTION_HORIZON = 10     # Paper: \"10-day ahead correlations\" (Section 4.1)\nTEST_DAYS = 60              # Out-of-sample test period\nEPOCHS = 100                # Training iterations\nLEARNING_RATE = 3e-4        # Paper: \"learning rate 3Ã—10^-4\" (Appendix A.1)\n\nprint(\"=\" * 70)\nprint(\"THGNN IMPLEMENTATION - PAPER-ALIGNED CONFIGURATION\")\nprint(\"=\" * 70)\nprint(f\"ğŸ“Š Universe: {len(TICKERS)} stocks\")\nprint(f\"ğŸ“… Lookback Window: {LOOKBACK_WINDOW} days (Paper: 30)\")\nprint(f\"ğŸ¯ Prediction Horizon: {PREDICTION_HORIZON} days (Paper: 10)\")\nprint(f\"ğŸ§ª Test Period: {TEST_DAYS} days\")\nprint(f\"âš™ï¸  Learning Rate: {LEARNING_RATE} (Paper: 3e-4)\")\nprint(\"=\" * 70)\n\n# =============================================================================\n# DATA ENGINEERING\n# =============================================================================\n# PAPER REFERENCE (Section 4.2.1 - Data):\n# \"For each S&P stock i at date t a feature vector x(i,t) âˆˆ R^F that includes\n# the following 37 features is constructed.\"\n#\n# Paper features include:\n# - Price/Volume: closing price, trading volume\n# - Technical: 5/20/60-day momentum, RSI-14, ATR-14, reversal\n# - Firm characteristics: market cap, book-to-market\n# - Factor exposures: FF3 betas (mkt, smb, hml)\n# - Macro: VIX, crude oil, 10Y Treasury, dollar index, GARCH volatility\n# - Returns: daily excess, raw, SPY return\n# - Sector codes: gsector, gsubind\n# - Correlation context: rolling market correlations\n#\n# For this demo, we implement a subset of these features.\n# =============================================================================\n\nprint(\"\\nğŸ“¥ FETCHING AND ENGINEERING FEATURES...\")\nraw_data = yf.download(TICKERS + [BENCHMARK], period=\"1y\", interval=\"1d\", progress=False)\n\n# Extract close prices\nclose_prices = raw_data['Close'][TICKERS]\n\n# =============================================================================\n# FEATURE ENGINEERING (Subset of Paper's 37 Features)\n# =============================================================================\n# 1. RETURNS - Daily percentage change\n#    Paper: \"daily excess return, raw return\"\nreturns = close_prices.pct_change().fillna(0)\n\n# 2. VOLATILITY - Rolling standard deviation\n#    Paper: \"GARCH(1,1) implied volatility\"\n#    Demo: Simple rolling volatility as proxy\nvolatility_5d = returns.rolling(window=5).std().fillna(0)\nvolatility_20d = returns.rolling(window=20).std().fillna(0)\n\n# 3. MOMENTUM - Rolling mean returns\n#    Paper: \"5/20/60-day momentum\"\nmomentum_5d = returns.rolling(window=5).mean().fillna(0)\nmomentum_20d = returns.rolling(window=20).mean().fillna(0)\n\n# 4. RSI-14 - Relative Strength Index (Simplified)\n#    Paper: \"relative strength index (RSI_14)\"\ndef compute_rsi(prices, window=14):\n    delta = prices.pct_change()\n    gain = delta.where(delta > 0, 0).rolling(window=window).mean()\n    loss = (-delta.where(delta < 0, 0)).rolling(window=window).mean()\n    rs = gain / (loss + 1e-10)\n    return rs / (1 + rs)  # Normalized to [0, 1]\n\nrsi_14 = compute_rsi(close_prices).fillna(0.5)\n\n# 5. MARKET CORRELATION - Rolling correlation with benchmark\n#    Paper: \"rolling correlations with the market (10, 21, and 63-day windows)\"\nbenchmark_returns = raw_data['Close'][BENCHMARK].pct_change().fillna(0)\nmarket_corr_21d = returns.rolling(window=21).corr(benchmark_returns).fillna(0)\n\n# --- Stack all features into tensor ---\n# Shape: [Time, Stocks, Features=7]\nfeature_names = ['Returns', 'Vol_5d', 'Vol_20d', 'Mom_5d', 'Mom_20d', 'RSI_14', 'Mkt_Corr']\nfeature_tensor = torch.stack([\n    torch.tensor(returns.values, dtype=torch.float32),\n    torch.tensor(volatility_5d.values, dtype=torch.float32),\n    torch.tensor(volatility_20d.values, dtype=torch.float32),\n    torch.tensor(momentum_5d.values, dtype=torch.float32),\n    torch.tensor(momentum_20d.values, dtype=torch.float32),\n    torch.tensor(rsi_14.values, dtype=torch.float32),\n    torch.tensor(market_corr_21d.values, dtype=torch.float32),\n], dim=2).to(device)\n\n# --- Normalization ---\n# Paper: \"all features are normalized using a rolling 60-day z-score\"\n# Demo: Simple standardization for efficiency\nfeature_mean = feature_tensor.mean(dim=0, keepdim=True)\nfeature_std = feature_tensor.std(dim=0, keepdim=True) + 1e-8\nfeature_tensor = (feature_tensor - feature_mean) / feature_std\n\nprint(f\"âœ… Features engineered: {feature_tensor.shape}\")\nprint(f\"   Shape: [Days={feature_tensor.shape[0]}, Stocks={feature_tensor.shape[1]}, Features={feature_tensor.shape[2]}]\")\nprint(f\"   Features: {feature_names}\")\n\n# --- Targets: Next-day returns ---\ntarget_tensor = torch.tensor(\n    returns.shift(-1).fillna(0).values, \n    dtype=torch.float32\n).to(device)\n\n# =============================================================================\n# GRAPH CONSTRUCTION WITH CORRELATION-BASED EDGES\n# =============================================================================\n# PAPER REFERENCE (Section 4.2.3):\n# \"Edge sampling: for every stock the top 50, bottom 50 correlations, and 75\n# randomly sampled mid-strength partners by Ï^base are included\"\n#\n# For this demo, we compute actual correlations to build edges.\n# =============================================================================\n\nprint(\"\\nğŸ”— BUILDING CORRELATION-BASED GRAPH...\")\n\n# Compute rolling 30-day correlation matrix\ncorr_matrix = returns.iloc[-30:].corr().values\n\n# Build edges from correlation matrix\nedges_src, edges_dst = [], []\nedge_weights = []\n\nfor i in range(len(TICKERS)):\n    # Get correlations for stock i (excluding self)\n    corrs = [(j, corr_matrix[i, j]) for j in range(len(TICKERS)) if i != j]\n    corrs.sort(key=lambda x: abs(x[1]), reverse=True)  # Sort by absolute correlation\n    \n    # Take top connected neighbors (simplified from paper's 50+50+75)\n    for j, corr in corrs[:4]:  # Top 4 neighbors per stock\n        edges_src.append(i)\n        edges_dst.append(j)\n        edge_weights.append(corr)\n\nedge_index = torch.tensor([edges_src, edges_dst], dtype=torch.long).to(device)\nedge_weights = torch.tensor(edge_weights, dtype=torch.float32).to(device)\n\nprint(f\"âœ… Graph built: {edge_index.shape[1]} edges\")\nprint(f\"   Average edge weight (correlation): {edge_weights.mean():.3f}\")\n\n# =============================================================================\n# FISHER-Z TRANSFORM\n# =============================================================================\n# PAPER REFERENCE (Section 4.1):\n# \"Correlations are bounded and heteroskedastic... Therefore to stabilize\n# variance and work on an approximately linear, unbounded scale, we apply\n# the Fisher transform: z = atanh(Ï)\"\n#\n# This is CRITICAL for proper correlation prediction as it:\n# 1. Maps bounded [-1,1] to unbounded (-âˆ,+âˆ)\n# 2. Stabilizes variance across correlation levels\n# 3. Makes gradients well-behaved\n# =============================================================================\n\ndef fisher_z_transform(rho):\n    \"\"\"\n    Fisher z-transform: z = arctanh(Ï)\n    \n    Maps correlation Ï âˆˆ [-1, 1] to z âˆˆ (-âˆ, +âˆ)\n    Paper (Section 4.1): \"stabilize variance and work on approximately\n    linear, unbounded scale\"\n    \"\"\"\n    # Clamp to avoid numerical issues at Â±1\n    rho_clamped = torch.clamp(rho, -0.999, 0.999)\n    return torch.atanh(rho_clamped)\n\ndef inverse_fisher_z(z):\n    \"\"\"\n    Inverse Fisher transform: Ï = tanh(z)\n    \n    Maps z âˆˆ (-âˆ, +âˆ) back to correlation Ï âˆˆ [-1, 1]\n    Paper: \"Z-space predictions are mapped back to correlations via tanh\"\n    \"\"\"\n    return torch.tanh(z)\n\n# Demonstrate Fisher-z transform\nprint(\"\\nğŸ“ FISHER-Z TRANSFORM DEMONSTRATION:\")\ndemo_corrs = torch.tensor([-0.9, -0.5, 0.0, 0.5, 0.9])\ndemo_z = fisher_z_transform(demo_corrs)\nprint(f\"   Correlations: {demo_corrs.tolist()}\")\nprint(f\"   Fisher-z:     {demo_z.tolist()}\")\nprint(f\"   Note: Extreme correlations spread out, stabilizing gradients\")\n\n# =============================================================================\n# PAPER-ACCURATE THGNN MODEL\n# =============================================================================\n# This implementation follows the paper's architecture more closely:\n#\n# TEMPORAL ENCODER (Section 4.2.2):\n# - 4-layer pre-norm Transformer\n# - 8 attention heads, d_model=128\n# - Output: 512-dim node embeddings\n#\n# RELATIONAL ENCODER (Section 4.2.4):\n# - 3 GAT layers, 4 heads each\n# - Edge-conditioned attention\n# - Relation-routed expert heads (neg/mid/pos)\n#\n# For computational efficiency, we scale down but preserve the structure.\n# =============================================================================\n\nclass PaperAccurateTHGNN(nn.Module):\n    \"\"\"\n    Temporal-Heterogeneous Graph Neural Network (THGNN)\n    \n    This model implements the key architectural concepts from the paper:\n    1. Transformer temporal encoder for per-stock time series\n    2. GAT relational encoder for cross-stock information flow\n    3. Residual prediction in Fisher-z space\n    \n    PAPER ARCHITECTURE COMPARISON:\n    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n    â”‚ Component          â”‚ Paper Spec      â”‚ Demo Spec           â”‚\n    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n    â”‚ d_model            â”‚ 128             â”‚ 64                  â”‚\n    â”‚ Transformer layers â”‚ 4               â”‚ 2                   â”‚\n    â”‚ Transformer heads  â”‚ 8               â”‚ 4                   â”‚\n    â”‚ GAT layers         â”‚ 3               â”‚ 2                   â”‚\n    â”‚ GAT heads          â”‚ 4               â”‚ 2                   â”‚\n    â”‚ Node embedding     â”‚ 512             â”‚ 64                  â”‚\n    â”‚ Features           â”‚ 37              â”‚ 7                   â”‚\n    â”‚ Window size        â”‚ 30              â”‚ 30                  â”‚\n    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n    \"\"\"\n    \n    def __init__(self, num_features, window_size, d_model=64, num_stocks=8):\n        super().__init__()\n        self.d_model = d_model\n        \n        # =====================================================================\n        # TEMPORAL ENCODER (Transformer)\n        # =====================================================================\n        # Paper (Section 4.2.2): \"Each feature sequence is mapped to d_model=128\n        # via a learned linear projection, with sinusoidal positional encodings\"\n        # =====================================================================\n        \n        # Input projection: [features] -> [d_model]\n        self.input_proj = nn.Linear(num_features, d_model)\n        \n        # Positional encoding (simplified from sinusoidal)\n        self.pos_encoding = nn.Parameter(torch.randn(1, window_size, d_model) * 0.02)\n        \n        # Transformer encoder stack\n        # Paper: \"4 pre-norm Transformer encoder layers with 8 attention heads\"\n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model=d_model,\n            nhead=4,                    # Paper: 8\n            dim_feedforward=d_model*4,  # Standard 4x expansion\n            dropout=0.2,                # Paper: \"dropout set to 0.2\"\n            batch_first=True,\n            norm_first=True             # Paper: \"pre-norm Transformer\"\n        )\n        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=2)  # Paper: 4\n        \n        # =====================================================================\n        # RELATIONAL ENCODER (Graph Attention Network)\n        # =====================================================================\n        # Paper (Section 4.2.4): \"We stack L_g=3 graph attention layers,\n        # each with H=4 attention heads\"\n        # =====================================================================\n        \n        # GAT layers with multi-head attention\n        self.gat1 = GATConv(d_model, d_model, heads=2, concat=False, dropout=0.2)\n        self.gat2 = GATConv(d_model, d_model, heads=2, concat=False, dropout=0.2)\n        \n        # Layer normalization after GAT (for stability)\n        self.gat_norm = nn.LayerNorm(d_model)\n        \n        # =====================================================================\n        # PREDICTION HEAD\n        # =====================================================================\n        # Paper (Section 4.2.4): \"We route u^edge_ij by the edge types to one\n        # of three small MLP expert heads\"\n        #\n        # Simplified: Single prediction head for next-day return\n        # Full paper predicts Î”z (Fisher-z residual) for each edge\n        # =====================================================================\n        \n        self.prediction_head = nn.Sequential(\n            nn.Linear(d_model, d_model // 2),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(d_model // 2, 1)\n        )\n    \n    def forward(self, x, edge_index, return_embeddings=False):\n        \"\"\"\n        Forward pass through THGNN.\n        \n        Args:\n            x: Input features [num_stocks, window_size, num_features]\n            edge_index: Graph edges [2, num_edges]\n            return_embeddings: If True, return node embeddings instead of predictions\n        \n        Returns:\n            predictions: [num_stocks, 1] or embeddings: [num_stocks, d_model]\n        \"\"\"\n        batch_size, seq_len, num_features = x.shape\n        \n        # =================================================================\n        # STAGE 1: TEMPORAL ENCODING\n        # =================================================================\n        # \"The Transformer processes each stock's historical feature sequence\n        # to produce latent node embeddings\" (Paper Figure 1)\n        # =================================================================\n        \n        # Project features to d_model dimension\n        x = self.input_proj(x)  # [num_stocks, window, d_model]\n        \n        # Add positional encoding\n        # Paper: \"sinusoidal positional encodings adding to preserve temporal order\"\n        x = x + self.pos_encoding[:, :seq_len, :]\n        \n        # Pass through Transformer\n        # Each stock independently processes its time series\n        x = self.transformer(x)  # [num_stocks, window, d_model]\n        \n        # Temporal pooling: take last timestep as summary\n        # Paper: \"flatten the matrix H(i,t) and apply LayerNorm-MLP\"\n        x_temporal = x[:, -1, :]  # [num_stocks, d_model]\n        \n        # =================================================================\n        # STAGE 2: RELATIONAL ENCODING\n        # =================================================================\n        # \"The GAT then focuses on the relational structure between assets,\n        # weighting neighbors via learned attention\" (Paper Section 3.5)\n        # =================================================================\n        \n        # First GAT layer\n        x_spatial = F.elu(self.gat1(x_temporal, edge_index))\n        \n        # Second GAT layer with residual connection\n        x_spatial = F.elu(self.gat2(x_spatial, edge_index)) + x_spatial\n        \n        # Layer normalization\n        x_spatial = self.gat_norm(x_spatial)\n        \n        if return_embeddings:\n            return x_spatial\n        \n        # =================================================================\n        # STAGE 3: PREDICTION\n        # =================================================================\n        return self.prediction_head(x_spatial)\n\n# =============================================================================\n# LOSS FUNCTION\n# =============================================================================\n# PAPER REFERENCE (Section 4.2.5):\n# \"We employ a Smooth-L1 (Huber) loss function that penalizes the difference\n# between predicted and realized Fisher values\"\n#\n# Huber Loss properties:\n# - Quadratic for small errors (sensitive)\n# - Linear for large errors (robust to outliers)\n# - Critical for financial data with regime shocks\n# =============================================================================\n\nclass HuberLoss(nn.Module):\n    \"\"\"\n    Huber Loss (Smooth L1) for robust regression.\n    \n    Paper (Section 4.2.5): \"Huber is less sensitive than MSE to large residuals\n    as it behaves quadratically for small residuals and linearly for large ones,\n    giving stable gradients near zero while being robust to regime shocks\"\n    \"\"\"\n    def __init__(self, delta=1.0):\n        super().__init__()\n        self.delta = delta\n    \n    def forward(self, pred, target):\n        abs_error = torch.abs(pred - target)\n        quadratic = torch.clamp(abs_error, max=self.delta)\n        linear = abs_error - quadratic\n        return torch.mean(0.5 * quadratic**2 + self.delta * linear)\n\n# =============================================================================\n# TRAINING LOOP\n# =============================================================================\nprint(\"\\n\" + \"=\" * 70)\nprint(\"TRAINING THGNN MODEL\")\nprint(\"=\" * 70)\n\n# Initialize model\nmodel = PaperAccurateTHGNN(\n    num_features=len(feature_names),\n    window_size=LOOKBACK_WINDOW,\n    d_model=64,\n    num_stocks=len(TICKERS)\n).to(device)\n\n# Optimizer: AdamW with weight decay\n# Paper (Appendix A.1): \"AdamW with learning rate 3Ã—10^-4, weight decay=2Ã—10^-4\"\noptimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=2e-4)\n\n# Learning rate scheduler\n# Paper: \"cosine decay schedule\"\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n\n# Loss function\ncriterion = HuberLoss(delta=1.0)\n\n# Train/test split\ntrain_size = feature_tensor.shape[0] - TEST_DAYS\n\nprint(f\"ğŸ“Š Training samples: {train_size - LOOKBACK_WINDOW}\")\nprint(f\"ğŸ“Š Test samples: {TEST_DAYS}\")\nprint(f\"ğŸ”§ Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n\n# Training loop\nmodel.train()\nlosses = []\n\nfor epoch in range(EPOCHS):\n    epoch_loss = 0\n    num_batches = 0\n    \n    # Sample random days from training period\n    for _ in range(10):  # 10 random samples per epoch\n        # Random day selection (must have enough history)\n        t = np.random.randint(LOOKBACK_WINDOW, train_size)\n        \n        # Extract window: [stocks, window, features]\n        x_window = feature_tensor[t-LOOKBACK_WINDOW:t].permute(1, 0, 2)\n        y_target = target_tensor[t].unsqueeze(1)\n        \n        # Forward pass\n        optimizer.zero_grad()\n        predictions = model(x_window, edge_index)\n        \n        # Compute loss\n        loss = criterion(predictions, y_target)\n        \n        # Backward pass\n        loss.backward()\n        \n        # Gradient clipping (Paper: \"clipped at 1.0\")\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        \n        optimizer.step()\n        \n        epoch_loss += loss.item()\n        num_batches += 1\n    \n    # Update learning rate\n    scheduler.step()\n    \n    avg_loss = epoch_loss / num_batches\n    losses.append(avg_loss)\n    \n    if (epoch + 1) % 20 == 0:\n        print(f\"   Epoch {epoch+1:3d}/{EPOCHS} | Loss: {avg_loss:.6f} | LR: {scheduler.get_last_lr()[0]:.2e}\")\n\nprint(\"âœ… Training complete!\")\n\n# Plot training loss\nplt.figure(figsize=(10, 4))\nplt.plot(losses, color='blue', alpha=0.7)\nplt.xlabel('Epoch')\nplt.ylabel('Huber Loss')\nplt.title('THGNN Training Loss')\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\n# =============================================================================\n# BACKTESTING\n# =============================================================================\n# PAPER REFERENCE (Section 4.3 - Trading):\n# \"Long positions are taken if a stock underperforms the cluster mean...\n# short positions are taken if a stock outperforms\"\n#\n# This implements a simplified long/short strategy based on model predictions.\n# =============================================================================\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"BACKTESTING: THGNN vs MARKET\")\nprint(\"=\" * 70)\n\nmodel.eval()\nportfolio_values = [1.0]\nbenchmark_values = [1.0]\ndaily_returns_strategy = []\ndaily_returns_market = []\n\nfor t in range(train_size, feature_tensor.shape[0] - 1):\n    # Get model prediction\n    x_window = feature_tensor[t-LOOKBACK_WINDOW:t].permute(1, 0, 2)\n    \n    with torch.no_grad():\n        predictions = model(x_window, edge_index).squeeze()\n    \n    # Strategy: Long top 2, Short bottom 2 (market neutral)\n    sorted_idx = torch.argsort(predictions, descending=True)\n    long_idx = sorted_idx[:2]\n    short_idx = sorted_idx[-2:]\n    \n    # Calculate returns\n    actual_returns = target_tensor[t]\n    long_return = actual_returns[long_idx].mean().item()\n    short_return = actual_returns[short_idx].mean().item()\n    \n    # Long/short spread return\n    strategy_return = long_return - short_return\n    market_return = actual_returns.mean().item()\n    \n    daily_returns_strategy.append(strategy_return)\n    daily_returns_market.append(market_return)\n    \n    portfolio_values.append(portfolio_values[-1] * (1 + strategy_return))\n    benchmark_values.append(benchmark_values[-1] * (1 + market_return))\n\n# Plot results\nplt.figure(figsize=(12, 5))\nplt.plot(portfolio_values, label='THGNN Strategy (Long/Short)', color='green', linewidth=2)\nplt.plot(benchmark_values, label='Market (Equal Weight)', color='gray', linestyle='--', linewidth=2)\nplt.xlabel('Trading Days')\nplt.ylabel('Cumulative Return')\nplt.title(f'THGNN Backtest Results ({TEST_DAYS} Days)')\nplt.legend()\nplt.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.show()\n\n# Performance metrics\nstrategy_return = (portfolio_values[-1] - 1) * 100\nmarket_return = (benchmark_values[-1] - 1) * 100\nsharpe_strategy = np.mean(daily_returns_strategy) / (np.std(daily_returns_strategy) + 1e-8) * np.sqrt(252)\nsharpe_market = np.mean(daily_returns_market) / (np.std(daily_returns_market) + 1e-8) * np.sqrt(252)\n\nprint(f\"\\nğŸ“ˆ PERFORMANCE SUMMARY:\")\nprint(f\"   Strategy Return: {strategy_return:+.2f}%\")\nprint(f\"   Market Return:   {market_return:+.2f}%\")\nprint(f\"   Strategy Sharpe: {sharpe_strategy:.2f}\")\nprint(f\"   Market Sharpe:   {sharpe_market:.2f}\")\nprint(f\"\\n   Paper Results (2019-2024): Sharpe 1.837 vs S&P 0.647\")",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 741
    },
    "id": "dYi3j_II9zvt",
    "outputId": "4a515d98-e56b-4847-d75d-190f7c2dd994"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# =============================================================================\n# CELL 3: RELATIVE STRENGTH TRADING SIMULATION\n# =============================================================================\n# This cell demonstrates how THGNN predictions translate into actionable trades.\n#\n# PAPER REFERENCE (Section 4.3 - Trading):\n# \"Long positions are taken if a stock underperforms the cluster mean (expected\n# to revert up); short positions are taken if a stock outperforms (expected to\n# revert down)... The ML classifier ensemble filters signals, taking only the\n# top 10% by predicted profitability.\"\n#\n# KEY CONCEPTS:\n# 1. RELATIVE RANKING - Model scores are normalized to [0,1] range\n#    - Highest score â†’ 100% â†’ Most bullish signal â†’ LONG\n#    - Lowest score â†’ 0% â†’ Most bearish signal â†’ SHORT\n#\n# 2. MARKET NEUTRAL - Long/short pairs cancel out market beta\n#    Paper: \"market-neutral by design... insulated from broad swings\"\n#\n# 3. POSITION SIZING - Fixed 10% of capital per trade\n#    Paper uses more sophisticated ML-filtered sizing\n# =============================================================================\n\ntrade_log = []\ncapital = 10000.0\ncurrent_capital = capital\n\nprint(\"=\" * 95)\nprint(\"RELATIVE STRENGTH TRADING SIMULATION\")\nprint(\"=\" * 95)\nprint(f\"\\nğŸ“Š Initial Capital: ${capital:,.2f}\")\nprint(f\"ğŸ“ˆ Strategy: Long highest-ranked, Short lowest-ranked (Market Neutral)\")\nprint(f\"ğŸ’° Position Size: 10% of capital per trade\\n\")\n\nprint(f\"{'DATE':<12} | {'TICKER':<6} | {'ACTION':<5} | {'RANKING':<10} | {'ENTRY ($)':<10} | {'EXIT ($)':<10} | {'PROFIT ($)':<10}\")\nprint(\"-\" * 95)\n\nmodel.eval()\n\n# =============================================================================\n# TRADING LOOP\n# =============================================================================\n# For each day in the test period:\n# 1. Extract feature window for all stocks\n# 2. Run THGNN to get alpha scores (predicted returns)\n# 3. Normalize scores to relative rankings\n# 4. Long the \"best\" stock, Short the \"worst\" stock\n# 5. Calculate P&L based on next-day actual returns\n# =============================================================================\n\nfor t in range(train_size, feature_tensor.shape[0]-1):\n    \n    # --- STEP 1: Prepare input data ---\n    # Extract LOOKBACK_WINDOW days of features for all stocks\n    # Shape: [num_stocks, window_size, num_features]\n    x_window = feature_tensor[t-LOOKBACK_WINDOW:t].permute(1, 0, 2)\n    \n    # --- STEP 2: Get model predictions ---\n    # These are \"alpha scores\" - higher means model expects higher returns\n    with torch.no_grad():\n        alpha_scores = model(x_window, edge_index).squeeze()\n    \n    # --- STEP 3: Relative Ranking (MinMax Scaling) ---\n    # PAPER INSIGHT: Rather than using raw scores, we rank stocks relative\n    # to each other. This forces a clear \"best\" and \"worst\" pick each day.\n    #\n    # Why relative ranking?\n    # - Raw model outputs may all be similar (e.g., all ~0.5)\n    # - Relative ranking forces differentiation\n    # - Aligns with paper's \"ML ensemble filters top 10%\" approach\n    min_score = alpha_scores.min()\n    max_score = alpha_scores.max()\n    \n    # Skip if all scores identical (no differentiation possible)\n    if max_score - min_score == 0:\n        continue\n    \n    # Scale to [0, 1]: 0 = worst predicted, 1 = best predicted\n    relative_probs = (alpha_scores - min_score) / (max_score - min_score)\n    \n    # --- STEP 4: Identify trade targets ---\n    best_idx = torch.argmax(relative_probs).item()   # LONG candidate\n    worst_idx = torch.argmin(relative_probs).item()  # SHORT candidate\n    \n    # Get prices\n    price_today = close_prices.iloc[t]\n    price_tomorrow = close_prices.iloc[t+1]\n    date_str = close_prices.index[t].strftime('%Y-%m-%d')\n    \n    # =================================================================\n    # EXECUTE LONG TRADE (Highest Ranked Stock)\n    # =================================================================\n    # Paper: \"Long positions are taken if a stock underperforms the\n    # cluster mean (expected to revert up)\"\n    # =================================================================\n    ticker = TICKERS[best_idx]\n    prob = relative_probs[best_idx].item()  # Will be 1.0 (best)\n    \n    entry_price = price_today[ticker]\n    exit_price = price_tomorrow[ticker]\n    \n    # Position sizing: 10% of current capital\n    position_size = current_capital * 0.10\n    shares = position_size / entry_price\n    \n    # Long profit = (Exit - Entry) Ã— Shares\n    profit = (exit_price - entry_price) * shares\n    current_capital += profit\n    \n    trade_log.append({\n        'Date': date_str, 'Ticker': ticker, 'Action': 'LONG',\n        'Prob': f\"{prob:.1%}\", 'Entry': entry_price,\n        'Exit': exit_price, 'PnL': profit\n    })\n    \n    print(f\"{date_str:<12} | {ticker:<6} | {'LONG':<5} | {prob:.1%}     | {entry_price:<10.2f} | {exit_price:<10.2f} | {profit:<+10.2f}\")\n    \n    # =================================================================\n    # EXECUTE SHORT TRADE (Lowest Ranked Stock)\n    # =================================================================\n    # Paper: \"Short positions are taken if a stock outperforms\n    # (expected to revert down)\"\n    # =================================================================\n    ticker = TICKERS[worst_idx]\n    prob = relative_probs[worst_idx].item()  # Will be 0.0 (worst)\n    short_prob = 1.0 - prob  # Confidence it's the worst = 100%\n    \n    entry_price = price_today[ticker]\n    exit_price = price_tomorrow[ticker]\n    \n    # Position sizing: 10% of current capital\n    position_size = current_capital * 0.10\n    shares = position_size / entry_price\n    \n    # Short profit = (Entry - Exit) Ã— Shares (profit when price drops)\n    profit = (entry_price - exit_price) * shares\n    current_capital += profit\n    \n    trade_log.append({\n        'Date': date_str, 'Ticker': ticker, 'Action': 'SHORT',\n        'Prob': f\"{short_prob:.1%}\", 'Entry': entry_price,\n        'Exit': exit_price, 'PnL': profit\n    })\n    \n    print(f\"{date_str:<12} | {ticker:<6} | {'SHORT':<5} | {short_prob:.1%}     | {entry_price:<10.2f} | {exit_price:<10.2f} | {profit:<+10.2f}\")\n\n# =============================================================================\n# SUMMARY STATISTICS\n# =============================================================================\nprint(\"-\" * 95)\ntotal_return = ((current_capital - capital) / capital) * 100\nprint(f\"\\nğŸ“Š TRADING SUMMARY:\")\nprint(f\"   Final Capital: ${current_capital:,.2f}\")\nprint(f\"   Total Return:  {total_return:+.2f}%\")\nprint(f\"   Total Trades:  {len(trade_log)}\")\n\n# Calculate win rate\nif trade_log:\n    wins = sum(1 for t in trade_log if t['PnL'] > 0)\n    print(f\"   Win Rate:      {wins/len(trade_log):.1%}\")\n    \nprint(f\"\\nğŸ“ NOTE: Paper achieves Sharpe 1.837 with full architecture, ML filtering,\")\nprint(f\"   and SPONGEsym clustering. This demo uses simplified components.\")",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n_4OTBgj90cm",
    "outputId": "51d198c6-c279-4d9f-b1b9-33ba10e4219f"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# =============================================================================\n# CELL 4: STATISTICAL ARBITRAGE WITH LATENT EMBEDDINGS\n# =============================================================================\n# This is the CORE INNOVATION of the paper: using GNN embeddings to discover\n# pairs that the AI predicts will be correlated, then trading their divergence.\n#\n# PAPER REFERENCE (Section 3.4 - Graph Attention):\n# \"Graph attention learns which neighbors contribute to each embedding...\n# high-attention weights indicate the model views those pairs as economically\n# connected, even if traditional correlation metrics miss the link.\"\n#\n# WHY LATENT EMBEDDINGS MATTER:\n# Traditional correlation: Computed from historical returns (backward-looking)\n# GNN embeddings: Learned representations of future relationships (forward-looking)\n#\n# The cosine similarity between embeddings reveals the AI's \"predicted\n# correlation\" - stocks with similar embeddings are expected to move together.\n#\n# STATISTICAL ARBITRAGE (STAT-ARB) STRATEGY:\n# 1. Find pairs with HIGH predicted correlation (AI says they're linked)\n# 2. Wait for PRICE DIVERGENCE (one moves while other doesn't)\n# 3. Bet on CONVERGENCE (mean reversion)\n#\n# Paper (Section 4.3): \"Long positions are taken if a stock underperforms\n# the cluster mean (expected to revert up); short positions if outperforms.\"\n# =============================================================================\n\nimport seaborn as sns\n\n# =============================================================================\n# STEP 1: EXTRACT LATENT EMBEDDINGS FROM TRAINED MODEL\n# =============================================================================\n# The key insight is to access the layer BEFORE the prediction head.\n# These embeddings are the stock's \"AI fingerprint\" - a dense vector that\n# captures all the features, temporal patterns, and graph relationships.\n#\n# Paper (Section 4.2.4): \"The node embedding h^(L)_i encodes the asset's\n# current state, informed by both its own history (Transformer) and its\n# network neighbors (GAT).\"\n# =============================================================================\n\nclass ResearchHybridTrader(nn.Module):\n    \"\"\"\n    Modified THGNN that can return intermediate embeddings.\n    \n    This allows us to access the latent space where stocks are represented\n    as vectors - similar vectors = AI predicts correlation.\n    \n    ARCHITECTURE:\n    Input â†’ Transformer â†’ GAT â†’ [Embeddings] â†’ Prediction Head\n                                    â†‘\n                            We extract these for pairs trading\n    \"\"\"\n    def __init__(self, num_features, window_size, d_model=64):\n        super().__init__()\n        \n        # Temporal encoding (same as main model)\n        self.input_proj = nn.Linear(num_features, d_model)\n        encoder_layer = nn.TransformerEncoderLayer(\n            d_model=d_model, \n            nhead=4, \n            dim_feedforward=d_model*2, \n            batch_first=True\n        )\n        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=2)\n        \n        # Relational encoding\n        self.gat = GATConv(d_model, d_model, heads=2, concat=False)\n        \n        # Prediction head\n        self.head = nn.Sequential(\n            nn.Linear(d_model, d_model//2), \n            nn.ReLU(), \n            nn.Linear(d_model//2, 1)\n        )\n    \n    def forward(self, x, edge_index, return_embeddings=False):\n        \"\"\"\n        Forward pass with optional embedding extraction.\n        \n        Args:\n            x: Input features [num_stocks, window, features]\n            edge_index: Graph edges\n            return_embeddings: If True, return d_model-dim vectors per stock\n        \n        Returns:\n            If return_embeddings=False: predictions [num_stocks, 1]\n            If return_embeddings=True: embeddings [num_stocks, d_model]\n        \"\"\"\n        # Project input to d_model dimensions\n        x = self.input_proj(x)\n        \n        # Transformer: capture temporal patterns\n        x = self.transformer(x)\n        \n        # Take last timestep as summary\n        x_summary = x[:, -1, :]  # [num_stocks, d_model]\n        \n        # GAT: incorporate neighbor information\n        x_spatial = self.gat(x_summary, edge_index)\n        x_spatial = F.elu(x_spatial)\n        \n        # Return embeddings for pairs analysis\n        if return_embeddings:\n            return x_spatial  # [num_stocks, d_model]\n        \n        # Or return predictions for trading\n        return self.head(x_spatial)\n\n# --- Transfer weights from trained model ---\nresearch_model = ResearchHybridTrader(\n    num_features=len(feature_names), \n    window_size=LOOKBACK_WINDOW\n).to(device)\n\n# Copy weights from the main model where architecture matches\nresearch_model.load_state_dict(model.state_dict(), strict=False)\nresearch_model.eval()\n\nprint(\"=\" * 70)\nprint(\"STATISTICAL ARBITRAGE: AI-PREDICTED CORRELATIONS\")\nprint(\"=\" * 70)\n\n# =============================================================================\n# STEP 2: COMPUTE AI-PREDICTED CORRELATION MATRIX\n# =============================================================================\n# The cosine similarity between embeddings is the AI's prediction of how\n# correlated two stocks will be.\n#\n# WHY COSINE SIMILARITY?\n# - Embeddings are high-dimensional vectors (64-dim in our case)\n# - Cosine similarity measures directional alignment: [-1, 1]\n# - High cosine = vectors point same direction = predicted correlation\n# - This is analogous to how the paper uses embeddings for clustering\n#\n# Paper (Section 4.3): \"SPONGEsym clustering forms groups based on the\n# correlation matrix... stocks in the same cluster are expected to co-move\"\n# =============================================================================\n\n# Extract embeddings for most recent data\nlast_window = feature_tensor[-LOOKBACK_WINDOW:].permute(1, 0, 2)\n\nwith torch.no_grad():\n    # Get d_model-dimensional embedding for each stock\n    embeddings = research_model(last_window, edge_index, return_embeddings=True)\n\n# Normalize embeddings for cosine similarity\n# ||v|| = 1 means dot product = cosine similarity\nemb_norm = F.normalize(embeddings, p=2, dim=1)\n\n# Compute pairwise cosine similarities (AI correlation matrix)\n# Matrix multiplication: (N, D) Ã— (D, N) = (N, N)\nai_correlation_matrix = torch.mm(emb_norm, emb_norm.t()).cpu().numpy()\n\nprint(f\"\\nğŸ“Š Extracted {embeddings.shape[1]}-dimensional embeddings for {len(TICKERS)} stocks\")\nprint(f\"ğŸ“ Computed {len(TICKERS)}Ã—{len(TICKERS)} AI correlation matrix\")\n\n# =============================================================================\n# STEP 3: VISUALIZE THE \"AI MIND MAP\"\n# =============================================================================\n# This heatmap shows which stocks the AI thinks are correlated.\n# High values (red) = AI predicts these stocks will move together\n# Low values (blue) = AI predicts low/negative correlation\n#\n# KEY INSIGHT: This is FORWARD-LOOKING correlation, not historical!\n# The AI has learned patterns from Transformer (temporal) and GAT (relational)\n# to predict future correlations.\n# =============================================================================\n\nplt.figure(figsize=(10, 8))\nsns.heatmap(\n    ai_correlation_matrix, \n    xticklabels=TICKERS, \n    yticklabels=TICKERS, \n    cmap='RdYlBu_r',  # Red = high, Blue = low\n    annot=True, \n    fmt=\".2f\",\n    vmin=-1, \n    vmax=1,\n    square=True\n)\nplt.title(\"GNN-Predicted Latent Correlations\\n(High Value = AI predicts co-movement)\", \n          fontsize=14, fontweight='bold')\nplt.figtext(0.5, 0.01, \n            \"Paper Insight: Latent embeddings capture forward-looking relationships missed by rolling correlations\",\n            ha='center', fontsize=10, style='italic')\nplt.tight_layout()\nplt.show()\n\n# =============================================================================\n# STEP 4: GENERATE STATISTICAL ARBITRAGE SIGNALS\n# =============================================================================\n# STAT-ARB LOGIC:\n# 1. Find pairs with HIGH AI-predicted correlation (>0.85)\n#    â†’ AI is confident these stocks should move together\n#\n# 2. Check for PRICE DIVERGENCE today\n#    â†’ One stock moved significantly more/less than the other\n#\n# 3. If divergence detected â†’ BET ON CONVERGENCE\n#    â†’ Short the outperformer, Long the underperformer\n#    â†’ Profit when they revert to their predicted relationship\n#\n# Paper (Section 4.3): \"The strategy is market-neutral by design... we\n# extract alpha from the spread rather than from directional moves\"\n# =============================================================================\n\nprint(\"\\n\" + \"=\" * 80)\nprint(\"INSTANT STAT-ARB SIGNALS (Pairs Trading)\")\nprint(\"=\" * 80)\nprint(f\"\\nLogic: Find high-correlation pairs (>85%) with today's divergence (>1.5%)\")\nprint(f\"Trade: Short outperformer + Long underperformer (bet on convergence)\\n\")\n\nprint(f\"{'PAIR':<12} | {'AI CORR':<10} | {'DIVERGENCE':<12} | {'SIGNAL':<25} | {'CONFIDENCE':<10}\")\nprint(\"-\" * 85)\n\n# Get today's actual returns\ncurrent_returns = returns.iloc[-1]\n\nsignals_found = 0\nfor i in range(len(TICKERS)):\n    for j in range(i + 1, len(TICKERS)):\n        stock_a = TICKERS[i]\n        stock_b = TICKERS[j]\n        \n        # AI Predicted Correlation (from embeddings)\n        ai_corr = ai_correlation_matrix[i, j]\n        \n        # Only consider pairs where AI predicts strong correlation\n        # Paper: \"SPONGEsym clustering... stocks in same cluster expected to co-move\"\n        if ai_corr > 0.85:\n            \n            # Check for price divergence today\n            # If they're supposed to be correlated but moved differently â†’ opportunity\n            ret_diff = current_returns[stock_a] - current_returns[stock_b]\n            \n            # Threshold: 1.5% divergence (significant gap)\n            if abs(ret_diff) > 0.015:\n                signals_found += 1\n                \n                # Trading signal:\n                # If A outperformed B â†’ Short A, Long B (expect A to drop / B to rise)\n                # If B outperformed A â†’ Short B, Long A (expect B to drop / A to rise)\n                if ret_diff > 0:\n                    signal = f\"SHORT {stock_a} / LONG {stock_b}\"\n                else:\n                    signal = f\"LONG {stock_a} / SHORT {stock_b}\"\n                \n                # Confidence based on correlation strength Ã— divergence magnitude\n                # Higher correlation + larger divergence = more confident mean reversion\n                confidence = ai_corr * min(abs(ret_diff) / 0.02, 1.0)\n                \n                print(f\"{stock_a}/{stock_b:<5} | {ai_corr:.4f}     | {ret_diff:+.2%}       | {signal:<25} | {confidence:.1%}\")\n\nif signals_found == 0:\n    print(\"   No significant divergence detected in high-correlation pairs today.\")\n    print(\"   (This is normal - stat-arb signals are not generated every day)\")\n\n# =============================================================================\n# SUMMARY: WHY THIS APPROACH IS POWERFUL\n# =============================================================================\nprint(\"\\n\" + \"=\" * 80)\nprint(\"WHY GNN-BASED STAT-ARB IS SUPERIOR TO TRADITIONAL METHODS\")\nprint(\"=\" * 80)\nprint(\"\"\"\nTRADITIONAL STAT-ARB:\n  â”œâ”€â”€ Uses rolling window correlations (e.g., 60-day)\n  â”œâ”€â”€ Backward-looking: reflects past regime, not current\n  â”œâ”€â”€ Slow to adapt: takes weeks to update during regime shifts\n  â””â”€â”€ Fails during crises: COVID spike took weeks to reflect\n\nTHGNN STAT-ARB (This Paper):\n  â”œâ”€â”€ Uses learned embeddings from Transformer + GAT\n  â”œâ”€â”€ Forward-looking: predicts future correlations\n  â”œâ”€â”€ Fast to adapt: updates daily with new information\n  â””â”€â”€ Crisis resilient: attention mechanism quickly identifies regime shifts\n\nPAPER RESULTS (Section 5):\n  â”œâ”€â”€ MAE reduced 25% (0.31 â†’ 0.23)\n  â”œâ”€â”€ Pearson r increased 151% (0.31 â†’ 0.78)\n  â”œâ”€â”€ Sharpe ratio: 1.837 vs 0.647 for S&P 500\n  â””â”€â”€ Max drawdown: -9.43% vs -33.93% for S&P 500\n\nKEY INSIGHT: \"Correlation should be treated as a predictable variable,\nnot a fixed historical summary\" (Paper Section 6)\n\"\"\")",
   "metadata": {
    "id": "1my6bSHK_2Yb",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 999
    },
    "outputId": "492baeca-e7db-46b1-a79f-0e737dc843fc"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "---\n\n## Summary: Key Takeaways from the THGNN Paper\n\n### The Core Problem\nTraditional statistical arbitrage relies on **rolling window correlations** (e.g., 60-day averages) which are:\n- **Backward-looking**: They reflect past relationships, not current ones\n- **Slow to adapt**: During regime shifts (like COVID-19), correlations spike within days but windowed estimators take weeks to update\n- **Prone to failure**: This lag causes collapsed trading baskets and significant losses\n\n### The THGNN Solution\nThe paper introduces a **Temporal-Heterogeneous Graph Neural Network** that combines:\n\n| Component | Purpose | Key Innovation |\n|-----------|---------|----------------|\n| **Transformer** | Encode per-stock time series | Captures temporal patterns within 30-day windows |\n| **GAT** | Model cross-stock relationships | Learns which neighbors matter via attention |\n| **Fisher-z Transform** | Stabilize correlation prediction | Maps bounded [-1,1] to unbounded space |\n| **Expert Heads** | Handle different correlation regimes | Routes neg/mid/pos correlations separately |\n\n### Results That Matter\n\n| Metric | Traditional | THGNN | Improvement |\n|--------|-------------|-------|-------------|\n| MAE | 0.307 | 0.230 | **-25%** |\n| Sharpe Ratio | 0.647 | 1.837 | **+184%** |\n| Max Drawdown | -33.93% | -9.43% | **+72%** |\n\n### For Practitioners\n1. **Replace rolling correlations** with forward-looking predictions\n2. **Feed macro variables** (VIX, rates, oil) to ensure regime adaptability\n3. **Use attention weights** as regime-shift indicators\n4. **Treat correlation as predictable**, not as a fixed historical summary\n\n---\n\n*Reference: [arXiv:2601.04602v1](https://arxiv.org/abs/2601.04602) - Fanshawe, Masih, Cameron (2026)*",
   "metadata": {
    "id": "zHH_sZA3IuQG"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}